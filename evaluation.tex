%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Evaluation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We evaluated Landslide in two ways.
First, we conducted a user study with present students of 15-410, to understand the eases and difficulties of Landslide's user interface process, to understand how to polish it into a useful debugging tool.
Second, we did a case study of six bugs in two kernels, investigating them in-depth to understand how systematic testing might best be harnessed to find bugs without in advance knowing the decision set necessary to expose them.

\section{User Experience}

To evaluate the experience of non-expert users working with Landslide, we met with 5 groups of students in 15-410 (9 students in total) during the final week of the kernel project, and had them use Landslide with their own kernels.
In order to garner student interest in Landslide, the author gave a lecture in 15-410 on systematic exploration in general and on Landslide specifically during the second-to-last week of the kernel project.\footnote{
The lecture slides are available at \url{http://www.cs.cmu.edu/~410-s12/lectures/L30_Landslide.pdf}, and at \url{http://bblum.net/landslide-lecture.pdf}, and at \url{http://www.contrib.andrew.cmu.edu/~bblum/landslide-lecture.pdf}.}

We wished to measure which phases of using Landslide are most time-consuming, so we asked students approximately every half-hour to record the amount of time they had spent doing each phase:

\begin{enumerate}
	\item Annotating their kernel (Section~\ref{sec:using-annotations}).
	\item Implementing instrumentation within Landslide (Section~\ref{sec:using-student-c}).
	\item Fixing problems encountered getting Landslide to run, such as editing their kernel to meet Landslide's requirements and debugging incorrect instrumentation.
	\item Customising Landslide's search parameters such as decision points (Section~\ref{sec:using-customise}).
	\item Analysing the output from bugs that Landslide found (Section~\ref{sec:using-interpreting}).
\end{enumerate}

We also wanted descriptive feedback, to evaluate what specific parts of the process of using Landslide need improved. We asked students to write brief remarks whenever they recorded amounts of time spent, and also after they were finished to answer the following questions.

\begin{itemize}
	\item Were you able to get Landslide to work with your kernel (i.e., run through a minimal exploration tree completely)? If so, how long did it take, from sitting down to getting it to work? If not, did your kernel do something that was incompatible with Landslide, or were you unable to get the instrumentation right for some other reason?
	\item Did you find bugs while using Landslide, that you imagine would have been very hard to find otherwise? Did Landslide's output help you understand what caused them?
	\item Were you able to get Landslide to say ``you survived!'' with custom decision points? Did you think the set of decision points you used for this provide a strong guarantee about the absence of races?
	\item Describe your experience configuring the set of decision points. What was intuitive, obvious to do? Did you feel stuck at any point, not knowing where to go next?
	\item Was there anything you wanted to make Landslide do that it didn't support?
\end{itemize}

Finally, we briefly studied each bug that the students found, which we describe below.

\subsection{Time Breakdown}

% TODO

\subsection{Descriptive Feedback}
\label{sec:eval-feedback}

The students provided specific remarks which fell into two categories: issues with Landslide's user interface, and needing to change something in one's kernel to make it work with Landslide.

\subsubsection{User Interface Feedback}

For Landslide to be appealing as a debugging tool, we should improve the user interface (or the infrastructure, in some cases) to address the following issues.

\begin{itemize}
	\item {\bf Difficulty of debugging incorrect instrumentation.} When the user-provided annotations/instrumentation are incorrect, Landslide's error messages are difficult to understand, and often don't point to the real problem.
	\item {\bf Interpreting debugging output.} Some students reported the decision trace representation could be better. One student reported the output was easy to follow by ``tracing the life of the buggy thread'', which is an insight into the learning process we should capitalise on.
	\item {\bf Ease of configuring decision sets.} Most students seemed to find the process of adding more decision points intuitive. One reported, ``quick and effective at detecting basic bugs/races.''
	\item {\bf General feature wishlist.} The following features, of varying difficulty to implement, would have improved user experience overall.
		\begin{itemize}
			\item Being able to continue exploration after interrupting Simics to work with the debug prompt, instead of having to start over.
			\item Support for multiprocessor kernels.
			\item Cutting down overall simulation time.
		\end{itemize}
\end{itemize}

\subsubsection{Technical Feedback}

Students also described compatibility issues they faced between their kernels and Landslide, which required them to make certain changes to their kernels.
% FIXME: Say how we will address these issues?

\begin{itemize}
	\item {\bf Special-case context switch behaviour.} One kernel had a special context-switcher for the case when a thread was vanishing, which had to be special-cased in the instrumentation.
		Some kernels had different context switch designs for just-forked threads (as discussed in the ``Thread Creation'' bullet in Section~\ref{sec:challenges-design}), and Landslide's code had to be modified during the study to support different designs.
	\item {\bf Unexpected compiler optimisations.} Listing scheduler functions in \texttt{config.landslide} could be confusing if the compiler inlined them.
	\item {\bf Driver thread interfering with test lifecycle tracking.} One kernel (Pathos, the 15-410 reference kernel) had to be modified to ensure the keyboard thread ran once before the test started, which was not mandated by the kernel design. (If the thread didn't run, it would conflict with the population-tracking described in Section~\ref{sec:components-test}, and Landslide would not be able to detect when the test was over.)
	\item {\bf Kernel behaviour requirement violations.}
		The majority of groups needed to change their scheduling around the idle thread in order to meet the requirement that the kernel never run idle when progress could otherwise be made (Sections~\ref{sec:challenges-design} and \ref{sec:using-requirements-sched}).
		Also, one kernel had to have its custom memory allocator disabled, because Landslide only supports LMM, the provided allocator.
\end{itemize}

Additionally, two enterprising users\footnote{The second case here was an ex-TA, not a student.} found ways to avoid implementing the functions in \texttt{student.c} (Section~\ref{sec:using-student-c}). It is worth considering integrating these approaches into the recommended approach to eliminate the need for \texttt{student.c} entirely.
\begin{itemize}
	\item One studnt bypassed the \texttt{kern\_ready\_for\_timer\_interrupt} annotation by disabling their scheduler's preemption-disabling mechanism and replacing it with explicit disabling of interrupts, thereby making every timer interrupt cause a context switch.
	\item One ex-TA was able to avoid the \texttt{kern\_current\_extra\_runnable} annotation by using the \texttt{on\_rq} and \texttt{off\_rq} annotations (Section~\ref{sec:using-annotations}) to express the ``abstract set of runnable threads'' (which included the currently-running thread, even though the runqueue didn't) instead of the literal runqueue itself.
\end{itemize}

\subsection{Landslide Victories}
% TODO
% nadim - wait bug (claimed easy to spot with code review)
% margaret/echo - ds assertion bug
% margaret/echo - vanish c_s return idle bug; helped find when idle was running when shouldn't be
% tpassaro found double free, but deterministic
% apodolsk found no bugs because no hello world
% pjumde's group found no bugs..?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bug Case Studies}
\label{sec:eval-casestudy}

\subsection{Bug Descriptions}

While building Landslide, we worked primarily with two kernels: one, the kernel the author wrote as a student in 15-410 (Fall 2008), and another, a kernel the author graded as a TA for 15-410 (Fall 2011). We refer to the first kernel as ``POBBLES'' and the second as ``LudicrOS''.

With Landslide, we found 4 bugs in POBBLES and 2 bugs in LudicrOS, and investigated them more deeply.

% TODO

\subsection{Numbers}
\label{sec:eval-numbers}

\newcommand\bugnum[2]{\textcolor{BrickRed}{{\bf #1} {\scriptsize \em (#2)}}}
\newcommand\nobugnum[2]{\textcolor{Blue}{{\em #1} {\scriptsize \em (#2)}}}

\begin{figure*}[t!]
	\begin{center}
	\small
	\begin{tabular}{|l||c|c|c|c||c|}
		\hline
		\multirow{2}{*}{\bf Kernel and test case} & \multicolumn{4}{c||}{\bf Decision points used} & \multirow{2}{*}{\bf Stress test} \\
		\cline{2-5}
		& \bf default & \bf lock & \bf unlock & \bf both & \\
		\hline\hline
		POBBLES vanish/vanish (a) & \nobugnum{31.8}{0.6} & \bugnum{57.1}{1.7} & $\infty$ & $\infty$ & \\
		\hline
		POBBLES vanish/vanish (b) & \nobugnum{32.0}{0.4} & \bugnum{51.5}{2.1} & \nobugnum{8057.9}{336.9} & $\infty$ & \\
		\hline
		POBBLES wait/wait & \bugnum{23.3}{0.7} & \bugnum{27.9}{0.8} & \bugnum{27.9}{2.1} & \bugnum{41.6}{1.4} & \\
		\hline
		POBBLES thread\_fork/vanish & \bugnum{22.0}{0.6} & \bugnum{37.4}{1.1} & \bugnum{27.6}{0.5} & \bugnum{72.0}{2.6} & \\
		\hline
		LudicrOS vanish/vanish & \nobugnum{13.2}{0.2} & \bugnum{13.7}{0.7} & \bugnum{34.6}{1.1} & \bugnum{17.1}{0.3} & \\
		\hline
		LudicrOS yield/vanish & \nobugnum{12.3}{0.3} & \bugnum{11.4}{0.4} & \nobugnum{27.4}{0.8} & \bugnum{11.7}{0.4} & \\
		\hline
		\multicolumn{6}{c}{Key: \bugnum{seconds}{stddev} indicates bug found; \nobugnum{seconds}{stddev} indicates whole tree explored with no bug.} \\
	\end{tabular}
	\end{center}
	\caption{Comparison of time taken (in seconds) to find bugs using Landslide and using conventional stress testing.
	Landslide's times are given for several different sets of decision points: the default set, consisting only of voluntary reschedules (Section~\ref{sec:components-arbiter}); and using custom decision points in addition to the default set: calls to \texttt{mutex\_lock}, calls to \texttt{mutex\_unlock}, and both.
	All numbers represent the average from 5 trials, with the standard deviations given in parentheses. ``$\infty$'' indicates the bug was not found with stress testing (after 1 hour), or that Landslide's search did not finish (after 8 hours). % TODO: what timeout for stress testing?
	}
	\label{fig:numbers}
\end{figure*}

% TODO: have a graph

Table~\ref{fig:numbers} shows the time it takes to find each of these bugs using Landslide, configured with several different sets of decision points, and using conventional stress testing.

The Landslide experimental set-up is as follows:

\begin{itemize}
	\item All Landslide trial times include the Simics start-up and kernel boot-up time (time between issuing the command and the test case beginning to run), roughly 15 seconds for POBBLES and 10 seconds for LudicrOS.
	\item All Landslide trials were run on the Gates-Hillman cluster machines (2.6 GHz Xeon).
	% TODO: say specs of crash machine
	\item All Landslide trials were run with ``backwards exploration'' enabled (Section~\ref{sec:using-search}).
	\item All trials were also run with Landslide configured to pay attention to only the relevant system calls (using \texttt{within\_function}; Section~\ref{sec:using-decision}).

	We believe it is reasonable to test for these bugs in this way - using the minimal set of system calls to be paid attention to as necessary to find the bug - because it follows the recommended workflow of using Landslide, which is to start with what the user judges to be the ``smallest relevant set'' of decision points. The configuration using \texttt{within\_function} was as follows.
	{\small
	\begin{itemize}
		\item POBBLES vanish/vanish(a): \texttt{within\_function vanish}
		\item POBBLES vanish/vanish(b): \texttt{within\_function vanish}
		\item POBBLES wait/wait: \texttt{within\_function wait}
		\item POBBLES thread\_fork/vanish: \texttt{within\_function thread\_fork} and \texttt{within\_function vanish}
		\item LudicrOS vanish/vanish: \texttt{within\_function vanish}
		\item LudicrOS yield/vanish: \texttt{within\_function yield} and \texttt{within\_function vanish}
	\end{itemize}
	}
\end{itemize}

%The stress test experimental set-up is as follows:
%
%\begin{itemize}
%	\item A wrapper program runs 1024 simultaneous copies of the same test program that Landslide uses. Whenever one of them exits, the wrapper starts a new one.
%	\item All stress test trials are on on the ``412 lab crashbox'' (3.2 GHz Pentium D).
%	\item The kernel and test case are configured to not print any messages during the test
%\end{itemize}

%%%%

\newcommand\bugtree[1]{\textcolor{BrickRed}{\bf #1}}
\newcommand\nobugtree[1]{\textcolor{Blue}{\em #1}}
\begin{figure*}[t!]
	\begin{center}
	\small
	\begin{tabular}{|l|l||c|c|c|c|}
		\hline
		\multirow{2}{*}{\bf Kernel and test case} & \multirow{2}{*}{\bf Property of tree} & \multicolumn{4}{|c|}{\bf Decision points used} \\
		\cline{3-6}
		& & \bf default & \bf lock & \bf unlock & \bf both \\
		\hline\hline
		\multirow{4}{*}{POBBLES vanish/vanish (a)} & Decision points & \nobugtree{56} & \bugtree{1296} & N/A & N/A \\
		& Total backtracks   & \nobugtree{16} & \bugtree{376} & N/A & N/A \\
		& Average branch depth & \nobugtree{5} & \bugtree{19} & N/A & N/A \\
		\hline
		\multirow{4}{*}{POBBLES vanish/vanish (b)} & Decision points & \nobugtree{56} & \bugtree{1295} & \nobugtree{382071} & N/A \\
		& Total backtracks   & \nobugtree{16} & \bugtree{376} & \nobugtree{112706} & N/A \\
		& Average branch depth & \nobugtree{5} & \bugtree{17} & \nobugtree{16} & N/A \\
		\hline
		\multirow{4}{*}{POBBLES wait/wait} & Decision points & \bugtree{23} & \bugtree{102} & \bugtree{74} & \bugtree{378} \\
		& Total backtracks   & \bugtree{4} & \bugtree{17} & \bugtree{12} & \bugtree{56} \\
		& Average branch depth & \bugtree{6} & \bugtree{10} & \bugtree{9} & \bugtree{14} \\
		\hline
		\multirow{4}{*}{POBBLES thread\_fork/vanish} & Decision points & \bugtree{24} & \bugtree{394} & \bugtree{273} & \bugtree{2269} \\
		& Total backtracks   & \bugtree{5} & \bugtree{70} & \bugtree{56} & \bugtree{410} \\
		& Average branch depth & \bugtree{5} & \bugtree{16} & \bugtree{14} & \bugtree{23} \\
		\hline
		\multirow{4}{*}{LudicrOS vanish/vanish} & Decision points & \nobugtree{10} & \bugtree{16} & \bugtree{141} & \bugtree{42} \\
		& Total backtracks   & \nobugtree{2} & \bugtree{3} & \bugtree{48} & \bugtree{10} \\
		& Average branch depth & \nobugtree{2} & \bugtree{7} & \bugtree{9} & \bugtree{14} \\
		\hline
		\multirow{4}{*}{LudicrOS yield/vanish} & Decision points & \nobugtree{8} & \bugtree{5} & \nobugtree{149} & \bugtree{7} \\
		& Total backtracks   & \nobugtree{1} & \bugtree{0} & \nobugtree{43} & \bugtree{0} \\
		& Average branch depth & \nobugtree{2} & \bugtree{0} & \nobugtree{9} & \bugtree{0} \\
		\hline
	\end{tabular}
	\end{center}
	\caption{Information about the decision trees explored when finding bugs. As in the previous table, each test case was run with the four different sets of decision points. ``\nobugtree{X}'' means the tree was completely explored because Landslide did not find a bug in that configuration. ``\bugtree{X}'' reflects the portion of the tree that was explored before a bug was found.}
	\label{fig:trees}
\end{figure*}

Table~\ref{fig:trees} presents more detailed information about the decision trees that Landslide explored when finding these bugs.
For each set of decision points on each bug, we give the total number of decision points in the tree, the total number of backtracks (i.e. branches explored before the bug was found), and the average branch depth (i.e. number of decision points in each branch).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Summary of Bugs Found}

% TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}

\subsection{Invariants}

While evaluating Landslide on these bugs, we determined two invariants that should hold for multiple explorations on the same test case.

\begin{enumerate}
	\item {\bf Ordering invariant.} For a given set of decision points, exploring the tree in multiple different orders (``forwards''/``backwards'') should produce the same result in terms of whether a bug was found or not. The bugs found may be different, and the number of branches explored may be different, but it should never be that one ordering finds a bug while another ordering of the same tree finds no bug.
	\item {\bf Superset invariant.} For a given set of decision points, if an exploration of the resulting tree finds a bug, using a superset of that set of decision points should also find a bug. This is because the first tree will be a sub-tree of the second, as shown by never preempting at a decision point that appears in the second set but not the first.
\end{enumerate}

In short, even though Landslide may give false negatives from using imperfect sets of decision points, the exploration itself must be sound (i.e. not missing any bugs that exist in the resulting tree).\footnote{
When running LudicrOS yield/vanish with decision points on \texttt{mutex\_unlock} but not on \texttt{mutex\_lock}, we found that the ordering invariant failed - ``backwards'' exploration found no bug, while ``forwards'' exploration did. We attribute this to a bug in Landslide itself, and present the results for the backwards exploration as usual, in which Landslide found no bug.}

\subsection{Recommended Testing Strategies}
\label{sec:discussion-strategies}

We make several observations about the experimental results from Section~\ref{sec:eval-numbers}.

\begin{enumerate}
	\item {\bf Fewer is faster.} While it is theoretically possible that a tree built of finer-grained interleavings might encounter a bug after fewer overall backtracks, we found that this did not happen in practice.\footnote{
		The one suspicious case is the LudicrOS vanish/vanish bug - exploring the ``both'' tree found the bug faster than exploring the ``unlock'' tree, despite the latter's decision points being a subset of the former's. However, we see that exploring the ``lock'' tree found the bug faster than either, so overall the ``both'' tree did not outperform the fastest of the smaller trees.}
		In general, for two sets of decision points that both find the same bug, the one that results in shorter {\em branches} will result in fewer {\em backtracks} needed to expose the bug, and hence less overall time.
	\item {\bf Different decision points are differently likely to expose different bugs.} We see that even though the ``lock'' and ``unlock'' trees tended to be about the same size, they were each sometimes better than the other at finding bugs. The ``lock'' tree did better on the vanish/vanish bugs and the yield/vanish bug, while the ``unlock'' tree did better on the wait/wait bug and the thread\_fork/vanish bug.\footnote{
	The latter two bugs needed no more than the default set to uncover at all, but we claim this still demonstrates the principle in general.}
	\item {\bf Finding a bug is fast, if it exists.} As especially exhibited in the POBBLES vanish/vanish (b) case, if two sets of decision points generate trees of approximately equal size, but one tree contains a bug and the other doesn't, then searching the bugful tree will likely terminate much more quickly.
	Of course, it is always possible that a bug may only exist in the very last branch of a tree, but we found that in general bugs show up early during exploration.
\end{enumerate}

% TODO: Investigate the tree size that bugs did exist in. Investigate how many buggy branches there were.

In light of these, we recommend several principles to govern an overall strategy to automatically iterate through different sets of decision points in search of a bug.\footnote{
We now say ``Landslide'' here to refer to a hypothetical test framework to embody these strategies, although of course it would not have to be named that.}

\begin{enumerate}
	\item {\bf Iterate exploring, starting with smaller decision sets.} To properly test for a bug in a particular test case, Landslide should try as many different decision sets as possible.
	The first exploration should always be just the default decision set, because that tends to complete quickly, and can help identify new decision points (Section~\ref{sec:future-analysis}).
	When Landslide has multiple decision sets as candidates for the next exploration, it should prefer to explore ones that result in shorter average branch depth.
	In this way, Landslide will tend to find bugs with the minimal decision set needed to expose them, in accordance with observation 1 above.
	\item {\bf Run multiple explorations at once.}
	As per observation 2, if Landslide has two decision sets that result in roughly equal average branch depth, it cannot know in advance whether either one will find a bug and/or finish significantly faster than the other. As such, it should try to run both explorations in parallel, wait for either one to finish, and continue iterating as appropriate even if the other one has not finished.
	\item {\bf De-prioritise longer-lasting test configurations.}
	With finite resources for parallelisation, Landslide should attempt to load-balance whatever searches are running in parallel according to each one's likelihood of finding a bug.
	In light of observation 3, if a particular search is running abnormally long for its average branch depth (the POBBLES vanish/vanish (b) bug with the ``unlock'' tree is a prime example), Landslide could judge that it is less likely to end soon with a positive result, and prioritise searches with other decision sets.
\end{enumerate}

% vim: ft=tex
